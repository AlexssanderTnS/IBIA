import streamlit as st
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama


embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-en-v1.5",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)

db = Chroma(persist_directory="DB", embedding_function=embeddings)



def gerar_resposta_ibIA(pergunta: str, contexto: str) -> str:
    llm = ChatOllama(
        model="phi3:mini", 
        temperature=0.2,
    )

    prompt = f"""
VocÃª Ã© a IBIA â€” InteligÃªncia Baseada em InstruÃ§Ã£o Automotiva.

Ã‰ uma instrutora virtual de trÃ¢nsito, especialista em CNH, legislaÃ§Ã£o, direÃ§Ã£o defensiva
e educaÃ§Ã£o para o trÃ¢nsito.

Regras:
- Responda SEMPRE em portuguÃªs do Brasil.
- Seja clara, simples, amigÃ¡vel e profissional.
- Use APENAS o CONTEXTO fornecido abaixo para responder.
- Se o contexto nÃ£o trouxer a resposta, diga claramente que o material nÃ£o Ã© suficiente
  e recomende procurar um instrutor ou material complementar.
- Explique em linguagem acessÃ­vel, como se estivesse conversando com um aluno.

------------------- CONTEXTO -------------------
{contexto}
------------------------------------------------

Pergunta do aluno:
{pergunta}

Resposta da IBIA:
"""

    resultado = llm.invoke(prompt)
    return getattr(resultado, "content", str(resultado))



st.set_page_config(page_title="IBIA - Assistente CNH", page_icon="ğŸš—")

st.title("IBIA - Assistente Virtual de CNH")


if "mensagens" not in st.session_state:
    st.session_state["mensagens"] = [
        {
            "role": "assistant",
            "content": (
                "OlÃ¡! Eu sou a **IBIA**, sua assistente virtual de educaÃ§Ã£o para o trÃ¢nsito. "
                "Posso te ajudar com dÃºvidas sobre CNH, leis de trÃ¢nsito, direÃ§Ã£o defensiva "
                "e conteÃºdos da sua apostila. O que vocÃª gostaria de saber hoje?"
            ),
        }
    ]

# Renderizar mensagens do histÃ³rico
for msg in st.session_state["mensagens"]:
    avatar = "assets/IBIA.png" if msg["role"] == "assistant" else "ğŸ‘¤"
    with st.chat_message(msg["role"], avatar=avatar):
        st.markdown(msg["content"])


# Quando o usuÃ¡rio envia uma pergunta
pergunta = st.chat_input("Digite sua dÃºvida sobre CNH:")

if pergunta:
    st.session_state["mensagens"].append(
        {"role": "user", "content": pergunta}
    )

    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(pergunta)

    resultados = db.similarity_search_with_score(pergunta, k=6)
    limite_score = 0.55
    relevantes = [(doc, score) for doc, score in resultados if score <= limite_score]

    if not relevantes:
        resposta = (
            "NÃ£o encontrei informaÃ§Ãµes suficientes nos materiais carregados..."
        )
    else:
        partes_contexto = [doc.page_content for doc, score in relevantes]
        contexto = "\n\n".join(partes_contexto)

        with st.chat_message("assistant", avatar="assets/IBIA.png"):
            with st.spinner("IBIA estÃ¡ pensando..."):
                try:
                    resposta = gerar_resposta_ibIA(pergunta, contexto)
                    st.markdown(resposta)
                except Exception as e:
                    resposta = f"Erro: `{e}`"
                    st.error(resposta)

    st.session_state["mensagens"].append(
        {"role": "assistant", "content": resposta}
    )
