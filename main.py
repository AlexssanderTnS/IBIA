import streamlit as st
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama


embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-en-v1.5",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)

db = Chroma(persist_directory="DB", embedding_function=embeddings)



def gerar_resposta_ibIA(pergunta: str, contexto: str) -> str:
    llm = ChatOllama(
        model="phi3:mini", 
        temperature=0.2,
    )

    prompt = f"""
Voc√™ √© a IBIA ‚Äî Intelig√™ncia Baseada em Instru√ß√£o Automotiva.

√â uma instrutora virtual de tr√¢nsito, especialista em CNH, legisla√ß√£o, dire√ß√£o defensiva
e educa√ß√£o para o tr√¢nsito.

Regras:
- Responda SEMPRE em portugu√™s do Brasil.
- Seja clara, simples, amig√°vel e profissional.
- Use APENAS o CONTEXTO fornecido abaixo para responder.
- Se o contexto n√£o trouxer a resposta, diga claramente que o material n√£o √© suficiente
  e recomende procurar um instrutor ou material complementar.
- Explique em linguagem acess√≠vel, como se estivesse conversando com um aluno.

------------------- CONTEXTO -------------------
{contexto}
------------------------------------------------

Pergunta do aluno:
{pergunta}

Resposta da IBIA:
"""

    resultado = llm.invoke(prompt)
    return getattr(resultado, "content", str(resultado))



st.set_page_config(page_title="IBIA - Assistente CNH", page_icon="üöó")

st.title("IBIA - Assistente Virtual de CNH")


if "mensagens" not in st.session_state:
    st.session_state["mensagens"] = [
        {
            "role": "assistant",
            "content": (
                "Ol√°! Eu sou a **IBIA**, sua assistente virtual de educa√ß√£o para o tr√¢nsito. "
                "Posso te ajudar com d√∫vidas sobre CNH, leis de tr√¢nsito, dire√ß√£o defensiva "
                "e conte√∫dos da sua apostila. O que voc√™ gostaria de saber hoje?"
            ),
        }
    ]

for msg in st.session_state["mensagens"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])


pergunta = st.chat_input("Digite sua d√∫vida sobre CNH:")

if pergunta:

    st.session_state["mensagens"].append(
        {"role": "user", "content": pergunta}
    )


    with st.chat_message("user"):
        st.markdown(pergunta)


    resultados = db.similarity_search_with_score(pergunta, k=6)
    limite_score = 0.55
    relevantes = [(doc, score) for doc, score in resultados if score <= limite_score]

    if not relevantes:
        resposta = (
            "N√£o encontrei informa√ß√µes suficientes nos materiais carregados para responder "
            "com seguran√ßa √† sua pergunta. Recomendo consultar seu instrutor ou material "
            "complementar da autoescola."
        )
    else:
        partes_contexto = [doc.page_content for doc, score in relevantes]
        contexto = "\n\n".join(partes_contexto)


        with st.chat_message("assistant"):
            with st.spinner("IBIA est√° pensando..."):
                try:
                    resposta = gerar_resposta_ibIA(pergunta, contexto)
                    st.markdown(resposta)
                except Exception as e:
                    resposta = f"Erro ao gerar resposta com o modelo local: `{e}`"
                    st.error(resposta)

    
    st.session_state["mensagens"].append(
        {"role": "assistant", "content": resposta}
    )
